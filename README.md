# Gesture Detection using YOLO

## Project Overview

This project focuses on detecting hand gestures using the YOLO (You Only Look Once) object detection model. The dataset consists of multiple hand gesture classes for both left and right hands, but only the right-hand (VFR) classes are used for training and evaluation. The goal is to develop a robust model capable of accurately identifying right-hand gestures in real-time applications.

## Dataset

The dataset contains five folders (S1-S5), each with:

- A folder with PNG image files.

- A corresponding folder with YOLO text annotation files.

Only the right-hand (VFR) classes are considered for training.

The dataset needs to be split into training and validation sets while managing storage constraints.
